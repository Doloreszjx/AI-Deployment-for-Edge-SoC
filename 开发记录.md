# phase 1： YOLOv8n Baseline 推理（Windows + CPU）
在Windows CPU环境下跑通YOLOv8你， 作为端侧部署的baseline
这个阶段不涉及任何SoC特定优化，主要用于建立后续模型转换，量化和算子优化的对比基线

**Q: 为什么不一开始就优化** </br>
因为端侧优化必须建立在可复现，可对比的baseline之上，否则无法量化每一步优化带来的收益

## 分析数据
### 数据1：
写入时间为：2026-01-06 16:19 \
Device: CPU \
Interface Time: 44.56 ms \
Memory Usage: 0.00 MB 

推理结果合理，YOLOv8n在Windows CPU上单张640X640， 40-60ms是常见的区间 \
**Memory Usage 为 0.00 MB** \
原因:
- psutil 读的是 RSS 增量
- YOLO 模型在：
初始化阶段已分配大部分内存;
单次 inference 不再显著增长
- Windows 内存回收策略 + Python GC \
→ 导致 delta ≈ 0

总结：
- 单次推理的内存增量在 Windows 上不明显，**模型参数**和**缓存**主要在*初始化阶段*完成分配，因此 RSS delta 接近 0，这是符合预期的。
- 真正端侧内存评估通常通过工具链统计 peak memory 或 allocator 级别数据，而非 Python 进程 delta

### 数据2：
Input Shape: (1, 3, 640, 480)

Preprocess:  1.6 ms
Inference:  33.7 ms
Postprocess: 0.9 ms

End-to-end: 41.97 ms \
Memory delta: 0.01 MB

从这组数据可以看到**推理阶段占主导**（interface around 80% of total latency)， 这是典型端侧SoC场景。说明瓶颈主要集中在模型算子执行阶段，而不是 I/O 或后处理。

#### 1 输入分辨率变化
输入的图片的实际尺寸为640X480，而非640X640，这意味着resize / letterbox 发生或memory access pattern 改变。 \
实际端侧视频流往往不是网络默认输入尺寸，因此我同时关注了 preprocess 对整体延迟的影响。

#### 2. 内存几乎不增长
0.01 MB 说明：
- 权重和 buffer 已复用
- 没有额外 graph rebuild
- 推理过程 memory footprint 稳定

这是部署质量好的信号。

# Phase2: 模型移植（PyTorch → ONNX）
数据对比

| Backend      | Device | Latency      |
| ------------ | ------ | ------------ |
| PyTorch      | CPU    | ~42–45 ms    |
| ONNX Runtime | CPU    | **37.00 ms** |

这说明什么？

ONNX Runtime 确实减少了 framework overhead
在 CPU-only 场景下：
- graph 优化
- operator scheduling

已经开始起作用

在端侧部署前，首先将 YOLOv8n 从训练框架 PyTorch 导出为 ONNX，作为与硬件无关的中间表示。
在 CPU-only 环境下使用 ONNX Runtime 进行推理验证，结果显示在 640×640 输入下，**推理延迟从约 44 ms 降至 37 ms**，说明 Runtime 层的图优化和算子调度已经带来收益，这也为后续对接 SoC 编译器或 NPU 后端打下基础。

**Q：这一步对 SoC 有什么意义？**

大多数 SoC 的 AI 工具链都是以 ONNX 或类似 IR 作为输入，因此这一步本质上是在验证模型是否“可被 SoC 消化”。

# Phase 3 分析ONNX Profile报告，得出下一步优化方向
YOLOv8n 在端侧 CPU 场景下呈现出典型的 Conv 计算主导 + 数据重排（layout reorder）显著 + 大量小算子碎片化的性能特征，瓶颈已从纯算力转向内存访问与算子调度效率。

## 1. Conv 占比 ≈ 61% ——“合理，但不够理想”
`Conv: 24.95 ms / ~41 ms ≈ 60.9%`

工程含义

- YOLOv8n 不是算子极端单一的模型
- Conv 仍是主力，但已经： 
  - 低于服务器模型（>80%） 
  - 说明端侧瓶颈开始外溢

**这说明 YOLOv8n 已经不是单纯 compute-bound，而是开始受到 memory layout 与算子调度的影响。**

## 2. ReorderInput + ReorderOutput ≈ 17%

```commandline
ReorderInput  ≈ 8.4%
ReorderOutput ≈ 8.4%
```

这是典型的 SoC / NPU 工程痛点：

数据在：

NCHW ↔ NHWC

blocked layout ↔ plain layout
之间频繁转换

转换本身 不做计算，只搬数据

**数据布局不一致，导致大量 memory-bound 的重排算子。**

## 3. Transpose + Concat + Split ——结构性开销
```commandline
Transpose: 4.4%
Concat + Split: ~7.1%
```

工程含义

YOLOv8n 的 head 结构：
- 多分支
- 多尺度

在端侧：
- 控制流和 shape 操作成本不可忽略

**在端侧，结构复杂度往往比参数量更影响延迟。**

## 4. 小算子比例 = 52% ——“调度瓶颈”已成主因

```commandline
156 / 300 operators < 50 μs
```

集中在
- Reorder*
- QuickGelu
- Add / Reshape / Slice

**当前瓶颈不在单个算子性能，而在算子粒度过细导致的调度与数据移动开销。**

**总结**

在 CPU-only 环境下对 YOLOv8n 的 ONNX graph 做 profiling 后发现，Conv 仍然是主要计算负载，占用约 61% 的推理时间，但同时也观察到 ReorderInput / ReorderOutput 等数据布局转换算子占用了接近 17% 的总延迟。

进一步分析发现，超过一半的算子为单次执行时间低于 50 微秒的小算子，这类算子本身计算量很小，但由于频繁调度和内存访问，整体带来了显著的性能损耗。这说明在端侧场景下，瓶颈已经从纯计算转向算子调度和内存带宽效率。

# Phase 4 INT8优化以及效果变差原因分析

| pytorch      | onnx runtim | INT8      |
| ------------ | ------ | ------------ |
| 44.82 ms      | 35.00 ms    |70.62 ms    |

可以明显的看到INT8优化后反而interface的时间变长了，这是因为现在的环境里（CPUExecutionProvider）没有VNNI / AVX512-VNNI和专用INT8 MAC单元
因此导致INT8 Conv不是原生指令级加速，反而需要**Quantize/Dequantize， 数据类型转换，额外的memory access =》 这些都是“纯开销**

### 我们来看一下INT8的优化原理：
用更少的数据位表示数，换取更快的计算和更少的内存占用。

FP32: 32位浮点数 → 精度高，但体积大，计算慢
INT8: 8位整数   → 精度低，但体积小，计算快

类比：
FP32 = 高清图片（10MB，加载慢）
INT8 = 缩略图（100KB，加载快）

#### 为什么INT8优化精度变低，不影响效果？
神经网络的权重和特征值范围分布比较小

权重值范围：[-2.5, 2.5] 占99%

但FP32能表示的范围：[-3.4e38, 3.4e38]

#### 计算加速原理
**内存带宽节省（主要加速来源）**

FP32卷积：input[4字节] × weight[4字节]

INT8卷积：input[1字节] × weight[1字节]

内存传输减少到1/4！
→ 更少的数据搬运时间
→ 更低的功耗

#### 为什么变慢了？

1. 模型小（3.5MB）→ 内存节省有限
2. CPU无VNNI → 计算反而更复杂
3. 单batch → 无法分摊量化开销
4. Reorder操作 → 额外开销

结果：量化开销 > 计算节省

这也验证了 INT8 优化在端侧场景下高度依赖底层 SoC 是否提供专用的 INT8 加速单元。
